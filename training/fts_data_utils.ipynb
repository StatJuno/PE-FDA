{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d72c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from skfda import FDataGrid\n",
    "from skfda.preprocessing.dim_reduction import FPCA\n",
    "from skfda.representation.basis import BSplineBasis\n",
    "from skfda.preprocessing.smoothing import BasisSmoother\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sktime.transformations.panel.rocket import Rocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341ea53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_with_last_row(series, fixed_rows):\n",
    "    series = np.array(series)\n",
    "    current_rows, columns = series.shape\n",
    "    if current_rows > fixed_rows:\n",
    "        return series[:fixed_rows, :]\n",
    "    elif current_rows < fixed_rows:\n",
    "        last_row = series[-1, :]\n",
    "        padding = np.tile(last_row, (fixed_rows - current_rows, 1))\n",
    "        return np.vstack((series, padding))\n",
    "    return series\n",
    "\n",
    "def load_segmented_time_series(input_dir, max_len=11):\n",
    "    x_data, y_data = [], []\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.endswith('.csv') and file_name != 'multivariate_time_series.csv':\n",
    "            class_label = file_name.split('_')[0]\n",
    "            df = pd.read_csv(os.path.join(input_dir, file_name))\n",
    "            time_series = df.iloc[:, 3:].values\n",
    "            time_series = pad_with_last_row(time_series, max_len)\n",
    "            x_data.append(time_series)\n",
    "            y_data.append(class_label)\n",
    "    return np.array(x_data), np.array(y_data)\n",
    "\n",
    "def load_univariate_series(input_dir, keypoint='LEFT_ELBOW', coord='y', max_len=11):\n",
    "    x_data, y_data = [], []\n",
    "    col = f\"{keypoint}_{coord}\"\n",
    "    for fn in os.listdir(input_dir):\n",
    "        if not fn.endswith('.csv'):\n",
    "            continue\n",
    "        df = pd.read_csv(os.path.join(input_dir, fn))\n",
    "        series = df[col].values.reshape(-1, 1)            # (timepoints, 1)\n",
    "        series = pad_with_last_row(series, max_len)      # pad to fixed length\n",
    "        x_data.append(series)                            # 각 샘플당 (max_len, 1)\n",
    "        y_data.append(fn.split('_')[0])\n",
    "    return np.array(x_data), np.array(y_data)            # shape: (n_samples, max_len, 1)\n",
    "\n",
    "def to_fd_list(x_data, apply_smoothing=False, n_basis=7):\n",
    "    fd_list = [FDataGrid(data_matrix=sample.T, grid_points=np.linspace(0, 1, sample.shape[0])) for sample in x_data]\n",
    "    if apply_smoothing:\n",
    "        basis = BSplineBasis(n_basis=n_basis)\n",
    "        smoother = BasisSmoother(basis=basis)\n",
    "        fd_list = [smoother.fit_transform(fd) for fd in fd_list]\n",
    "    return fd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "746a2132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROCKET + FDA Accuracy: 0.8207547169811321\n"
     ]
    }
   ],
   "source": [
    "segment_dir = \"/Users/junokim/Desktop/jupyteryong2/slr/peak_detection_results/segments2\"\n",
    "# 1) 데이터 로드 + 패딩\n",
    "x_data, y_data = load_segmented_time_series(segment_dir, max_len=300)\n",
    "\n",
    "# 2) 함수형 객체로 변환 + 스무딩\n",
    "grid = np.linspace(0, 1, 300)\n",
    "fd = FDataGrid(data_matrix=x_data, grid_points=[grid])\n",
    "smoother = BasisSmoother(basis=BSplineBasis(n_basis=30))\n",
    "fd_smooth = smoother.fit_transform(fd)\n",
    "\n",
    "# 3) numpy로 꺼내서 축 순서 맞추기\n",
    "#    original: (n_samples, timepoints, features)\n",
    "# ROCKET expects: (n_samples, channels, timepoints)\n",
    "X = np.transpose(fd_smooth.data_matrix, (0, 1, 2))\n",
    "\n",
    "# 4) train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_data, test_size=0.2, stratify=y_data, random_state=42\n",
    ")\n",
    "\n",
    "# 5) ROCKET 변환 + Ridge 분류\n",
    "rocket = Rocket(num_kernels=5000, normalise=False)\n",
    "rocket.fit(X_train)\n",
    "X_train_tr = rocket.transform(X_train)\n",
    "X_test_tr  = rocket.transform(X_test)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_tr)\n",
    "X_test_scaled  = scaler.transform(X_test_tr)\n",
    "\n",
    "clf = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 6) 평가\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "print(\"ROCKET + FDA Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# 7) 모델 저장\n",
    "model_bundle = {\n",
    "    \"rocket\": rocket,\n",
    "    \"scaler\": scaler,\n",
    "    \"classifier\": clf,\n",
    "    \"smoother\": smoother,\n",
    "    \"grid\": grid\n",
    "}\n",
    "\n",
    "with open(\"rocket_fda_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_bundle, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a667f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 세그먼트 로드\n",
    "segment_dir = \"/Users/junokim/Desktop/jupyteryong2/slr/peak_detection_results/segments2\"\n",
    "x_data, y_data = load_segmented_time_series(segment_dir, max_len=11)\n",
    "\n",
    "# FDataGrid로 변환\n",
    "x_data = np.array(x_data)\n",
    "x_data = np.transpose(x_data, (0, 2, 1))  # (samples, features, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65fc5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data shape: (5296, 300, 36)\n",
      "Example entry shape: (300, 36)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_data shape:\", x_data.shape)\n",
    "print(\"Example entry shape:\", x_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "950aa568",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mx_data\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(x_data.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6dfdc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"/Users/junokim/Desktop/jupyteryong2/slr/test\"\n",
    "predictions = []\n",
    "\n",
    "for file_name in os.listdir(test_dir):\n",
    "    if file_name.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join(test_dir, file_name))\n",
    "        time_series = df.iloc[:, 3:].values\n",
    "        time_series = pad_with_last_row(time_series, fixed_rows=11)\n",
    "        fd = FDataGrid(data_matrix=time_series.T, grid_points=np.linspace(0, 1, 11))\n",
    "        basis = BSplineBasis(n_basis=7)\n",
    "        smoother = BasisSmoother(basis=basis)\n",
    "        fd = smoother.fit_transform(fd)\n",
    "        pred = model.predict([fd])[0]\n",
    "        predictions.append((file_name, pred))\n",
    "\n",
    "label_meaning = {\n",
    "    '377': '올바른 사이드 레터럴 레이즈 자세',\n",
    "    '378': '무릎 반동이 있는 자세',\n",
    "    '379': '어깨 으쓱하는 자세',\n",
    "    '380': '상완/전완 각도 고정 안된 자세',\n",
    "    '381': '손목 각도 고정 안된 자세',\n",
    "    '382': '상체 반동이 있는 자세'\n",
    "}\n",
    "\n",
    "for fname, label in predictions:\n",
    "    print(f\"{fname} → {label} ({label_meaning.get(label, '알 수 없음')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad6978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skfda import FDataGrid\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def mfpca_via_univariate(fd: FDataGrid,\n",
    "                         n_components_global: int,\n",
    "                         n_components_univariate: int):\n",
    "    \"\"\"\n",
    "    간단 MFPCA 구현: 각 차원별로 FPCA → 스코어 병합 → PCA\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fd : FDataGrid\n",
    "        data_matrix shape = (n_samples, n_timepoints, n_features)\n",
    "    n_components_global : int\n",
    "        최종 PCA 성분 개수 (MFPCA 축 개수)\n",
    "    n_components_univariate : int\n",
    "        각 차원별 FPCA에서 뽑을 성분 개수\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    fps : list of FPCA\n",
    "        각 피처별 FPCA 객체\n",
    "    pca : PCA\n",
    "        스코어 병합 후 적용된 전역 PCA 객체\n",
    "    global_scores : ndarray, shape (n_samples, n_components_global)\n",
    "        최종 MFPCA 점수\n",
    "    \"\"\"\n",
    "    # 1) Univariate FPCA for each feature\n",
    "    n_samples, n_timepoints, n_features = fd.data_matrix.shape\n",
    "    grids = fd.grid_points[0]  # length = n_timepoints\n",
    "\n",
    "    fps = []\n",
    "    scores_list = []\n",
    "    for j in range(n_features):\n",
    "        # j번째 feature만 골라 FDataGrid 생성\n",
    "        data_j = fd.data_matrix[:, :, j]           # (n_samples, n_timepoints)\n",
    "        fd_j   = FDataGrid(data_matrix=data_j,     # FDataGrid expects shape (samples, timepoints)\n",
    "                           grid_points=[grids])\n",
    "        fpca_j = FPCA(n_components=n_components_univariate)\n",
    "        fpca_j.fit(fd_j)\n",
    "        scores_j = fpca_j.transform(fd_j)         # (n_samples, n_components_univariate)\n",
    "        \n",
    "        fps.append(fpca_j)\n",
    "        scores_list.append(scores_j)\n",
    "    \n",
    "    # 2) 스코어 병합\n",
    "    # shape → (n_samples, n_features * n_components_univariate)\n",
    "    S = np.hstack(scores_list)\n",
    "    \n",
    "    # 3) Global PCA (MFPCA)\n",
    "    pca = PCA(n_components=n_components_global)\n",
    "    global_scores = pca.fit_transform(S)\n",
    "    \n",
    "    return fps, pca, global_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea476b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FtsClassifier:\n",
    "    def __init__(self, n_components=5):\n",
    "        self.classifiers_mapping = {}\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit(self, fd_train, y_train):\n",
    "        fpca = FPCA(n_components=self.n_components)\n",
    "        fpca.fit(fd_train)\n",
    "        scores = fpca.transform(fd_train)\n",
    "        clf = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "        clf.fit(scores, y_train)\n",
    "        self.classifiers_mapping['fpca'] = fpca\n",
    "        self.classifiers_mapping['classifier'] = clf\n",
    "\n",
    "    def predict(self, fd_list):\n",
    "        fpca = self.classifiers_mapping['fpca']\n",
    "        clf = self.classifiers_mapping['classifier']\n",
    "        scores = fpca.transform(fd_list)\n",
    "        return clf.predict(scores)\n",
    "\n",
    "    def evaluate(self, fd_test, y_test):\n",
    "        y_pred = self.predict(fd_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"📊 Accuracy: {acc:.4f}\")\n",
    "        print(\"🧮 Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "        print(\"📝 Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a505502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2009433962264151\n"
     ]
    }
   ],
   "source": [
    "from fts_data_utils import load_segmented_time_series, pad_with_last_row\n",
    "from skfda import FDataGrid\n",
    "from skfda.preprocessing.smoothing import BasisSmoother\n",
    "from skfda.representation.basis import BSplineBasis\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1) 데이터 로드\n",
    "x_data, y_data = load_segmented_time_series(\"/Users/junokim/Desktop/jupyteryong2/slr/peak_detection_results/segments2\", max_len=11)\n",
    "# x_data.shape = (n_samples, 11, n_features)\n",
    "\n",
    "# 2) 함수형 객체로 변환 + 스무딩\n",
    "grid = np.linspace(0, 1, 11)\n",
    "fd_all = FDataGrid(data_matrix=x_data, grid_points=[grid])\n",
    "smoother = BasisSmoother(basis=BSplineBasis(n_basis=30))\n",
    "fd_all = smoother.fit_transform(fd_all)\n",
    "\n",
    "# 3) train/test split\n",
    "idx = np.arange(len(y_data))\n",
    "tr, te, y_tr, y_te = train_test_split(idx, y_data, test_size=0.2, stratify=y_data)\n",
    "fd_tr, fd_te = fd_all[tr], fd_all[te]\n",
    "\n",
    "# 4) MFPCA via univariate\n",
    "fps, pca, X_tr = mfpca_via_univariate(fd_tr,\n",
    "                                      n_components_global=300,\n",
    "                                      n_components_univariate=10)\n",
    "X_te = pca.transform(np.hstack([\n",
    "    fps[j].transform(FDataGrid(fd_te.data_matrix[:,:,j], [grid]))\n",
    "    for j in range(fd_te.data_matrix.shape[2])\n",
    "]))\n",
    "\n",
    "# 5) 분류기 학습/평가\n",
    "clf = RidgeClassifierCV(alphas=np.logspace(-3,3,10))\n",
    "clf.fit(X_tr, y_tr)\n",
    "y_pred = clf.predict(X_te)\n",
    "print(\"Accuracy:\", accuracy_score(y_te, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd3bed",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[67], line 14\u001b[0m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# … (앞 부분은 그대로) …\u001b[39;00m\n",
      "\u001b[1;32m      7\u001b[0m \n",
      "\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 4) MFPCA via univariate\u001b[39;00m\n",
      "\u001b[1;32m      9\u001b[0m fps, pca, X_tr \u001b[38;5;241m=\u001b[39m mfpca_via_univariate(\n",
      "\u001b[1;32m     10\u001b[0m     fd_tr,\n",
      "\u001b[1;32m     11\u001b[0m     n_components_global\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n",
      "\u001b[1;32m     12\u001b[0m     n_components_univariate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n",
      "\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;32m---> 14\u001b[0m X_te \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mtransform(np\u001b[38;5;241m.\u001b[39mhstack([\n",
      "\u001b[1;32m     15\u001b[0m     fps[j]\u001b[38;5;241m.\u001b[39mtransform(\n",
      "\u001b[1;32m     16\u001b[0m         FDataGrid(fd_te\u001b[38;5;241m.\u001b[39mdata_matrix[:, :, j], [grid])\n",
      "\u001b[1;32m     17\u001b[0m     )\n",
      "\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(fd_te\u001b[38;5;241m.\u001b[39mdata_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[1;32m     19\u001b[0m ]))\n",
      "\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 5) Grid Search + SVM 분류기 학습/평가\u001b[39;00m\n",
      "\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# --------------------------------------\u001b[39;00m\n",
      "\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 파이프라인 정의\u001b[39;00m\n",
      "\u001b[1;32m     24\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n",
      "\u001b[1;32m     25\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m\"\u001b[39m, StandardScaler()),            \u001b[38;5;66;03m# 스코어 정규화\u001b[39;00m\n",
      "\u001b[1;32m     26\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvc\u001b[39m\u001b[38;5;124m\"\u001b[39m,    SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;32m     27\u001b[0m ])\n",
      "\n",
      "Cell \u001b[0;32mIn[67], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# … (앞 부분은 그대로) …\u001b[39;00m\n",
      "\u001b[1;32m      7\u001b[0m \n",
      "\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 4) MFPCA via univariate\u001b[39;00m\n",
      "\u001b[1;32m      9\u001b[0m fps, pca, X_tr \u001b[38;5;241m=\u001b[39m mfpca_via_univariate(\n",
      "\u001b[1;32m     10\u001b[0m     fd_tr,\n",
      "\u001b[1;32m     11\u001b[0m     n_components_global\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n",
      "\u001b[1;32m     12\u001b[0m     n_components_univariate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n",
      "\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[1;32m     14\u001b[0m X_te \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mtransform(np\u001b[38;5;241m.\u001b[39mhstack([\n",
      "\u001b[1;32m     15\u001b[0m     fps[j]\u001b[38;5;241m.\u001b[39mtransform(\n",
      "\u001b[0;32m---> 16\u001b[0m         \u001b[43mFDataGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfd_te\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     17\u001b[0m     )\n",
      "\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(fd_te\u001b[38;5;241m.\u001b[39mdata_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[1;32m     19\u001b[0m ]))\n",
      "\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 5) Grid Search + SVM 분류기 학습/평가\u001b[39;00m\n",
      "\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# --------------------------------------\u001b[39;00m\n",
      "\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 파이프라인 정의\u001b[39;00m\n",
      "\u001b[1;32m     24\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n",
      "\u001b[1;32m     25\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m\"\u001b[39m, StandardScaler()),            \u001b[38;5;66;03m# 스코어 정규화\u001b[39;00m\n",
      "\u001b[1;32m     26\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvc\u001b[39m\u001b[38;5;124m\"\u001b[39m,    SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;32m     27\u001b[0m ])\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/skfda/representation/grid.py:174\u001b[0m, in \u001b[0;36mFDataGrid.__init__\u001b[0;34m(self, data_matrix, grid_points, sample_points, domain_range, dataset_name, argument_names, coordinate_names, sample_names, extrapolation, interpolation)\u001b[0m\n",
      "\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_points \u001b[38;5;241m=\u001b[39m _to_grid_points([\n",
      "\u001b[1;32m    166\u001b[0m         np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_matrix\u001b[38;5;241m.\u001b[39mshape[i])\n",
      "\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_matrix\u001b[38;5;241m.\u001b[39mndim)\n",
      "\u001b[1;32m    168\u001b[0m     ])\n",
      "\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# Check that the dimension of the data matches the grid_points\u001b[39;00m\n",
      "\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# list\u001b[39;00m\n",
      "\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_points \u001b[38;5;241m=\u001b[39m \u001b[43m_to_grid_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid_points\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    176\u001b[0m     data_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m: \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_domain]\n",
      "\u001b[1;32m    177\u001b[0m     grid_points_shape \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_points]\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/skfda/_utils/_utils.py:146\u001b[0m, in \u001b[0;36m_to_grid_points\u001b[0;34m(grid_points_like)\u001b[0m\n",
      "\u001b[1;32m    143\u001b[0m     unidimensional \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unidimensional:\n",
      "\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43m_int_to_real\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid_points_like\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,)\n",
      "\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(_int_to_real(np\u001b[38;5;241m.\u001b[39masarray(i)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m grid_points_like)\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/skfda/_utils/_utils.py:552\u001b[0m, in \u001b[0;36m_int_to_real\u001b[0;34m(array)\u001b[0m\n",
      "\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(array\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger):\n",
      "\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n",
      "\u001b[0;32m--> 552\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(array\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mfloating)\n",
      "\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(NDArrayFloat, array)\n",
      "\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# … (앞 부분은 그대로) …\n",
    "\n",
    "# 4) MFPCA via univariate\n",
    "fps, pca, X_tr = mfpca_via_univariate(\n",
    "    fd_tr,\n",
    "    n_components_global=300,\n",
    "    n_components_univariate=10\n",
    ")\n",
    "X_te = pca.transform(np.hstack([\n",
    "    fps[j].transform(\n",
    "        FDataGrid(fd_te.data_matrix[:, :, j], [grid])\n",
    "    )\n",
    "    for j in range(fd_te.data_matrix.shape[2])\n",
    "]))\n",
    "\n",
    "# 5) Grid Search + SVM 분류기 학습/평가\n",
    "# --------------------------------------\n",
    "# 파이프라인 정의\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),            # 스코어 정규화\n",
    "    (\"svc\",    SVC(kernel=\"rbf\", class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "# 탐색할 하이퍼파라미터 그리드\n",
    "param_grid = {\n",
    "    \"svc__C\":     [0.1, 1, 10],\n",
    "    \"svc__gamma\": [\"scale\", 0.01, 0.1],\n",
    "}\n",
    "\n",
    "# 5겹 교차검증 GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=1,        # 멀티코어 병렬화\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 학습\n",
    "grid.fit(X_tr, y_tr)\n",
    "\n",
    "# 최고 파라미터와 CV 점수 출력\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV accuracy:\", grid.best_score_)\n",
    "\n",
    "# 테스트 세트 성능\n",
    "test_acc = grid.score(X_te, y_te)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "899b8294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.14433962264150943\n"
     ]
    }
   ],
   "source": [
    "from skfda import FDataGrid\n",
    "from skfda.representation.basis import BSplineBasis\n",
    "from skfda.preprocessing.smoothing import BasisSmoother\n",
    "import numpy as np\n",
    "\n",
    "segment_dir = \"/Users/junokim/Desktop/jupyteryong2/slr/peak_detection_results/segments2\"\n",
    "# 1) 데이터 로드 + 패딩\n",
    "x_data, y_data = load_segmented_time_series(segment_dir, max_len=100)\n",
    "\n",
    "# x_data.shape = (n_samples, timepoints, n_features)\n",
    "grid = np.linspace(0, 1, 100)\n",
    "\n",
    "# 1-1) FDataGrid 생성\n",
    "fd = FDataGrid(data_matrix=x_data, grid_points=[grid])\n",
    "\n",
    "# 1-2) (선택) B-spline 스무딩\n",
    "basis    = BSplineBasis(n_basis=9)\n",
    "smoother = BasisSmoother(basis=basis)\n",
    "fd_smooth = smoother.fit_transform(fd)\n",
    "\n",
    "from skfda.preprocessing.dim_reduction import FPCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def mfpca_via_univariate(fd, n_univ=5, n_global=20):\n",
    "    # fd.data_matrix.shape == (n_samples, timepoints, n_features)\n",
    "    n_features = fd.data_matrix.shape[2]\n",
    "    grids      = fd.grid_points[0]\n",
    "\n",
    "    # 2-1) 각 피처별 FPCA → 스코어 계산\n",
    "    fps = []\n",
    "    scores = []\n",
    "    for j in range(n_features):\n",
    "        # j번째 채널만 뽑아 FDataGrid 객체 생성\n",
    "        fd_j = FDataGrid(data_matrix=fd.data_matrix[:, :, j],\n",
    "                         grid_points=[grids])\n",
    "        fpca_j = FPCA(n_components=n_univ)\n",
    "        fpca_j.fit(fd_j)\n",
    "        S_j = fpca_j.transform(fd_j)             # (n_samples, n_univ)\n",
    "        fps.append(fpca_j)\n",
    "        scores.append(S_j)\n",
    "\n",
    "    # 2-2) 스코어 합치기 → (n_samples, n_features*n_univ)\n",
    "    import numpy as np\n",
    "    S = np.hstack(scores)\n",
    "\n",
    "    # 2-3) Global PCA → (n_samples, n_global)\n",
    "    pca = PCA(n_components=n_global)\n",
    "    G = pca.fit_transform(S)\n",
    "\n",
    "    return fps, pca, G\n",
    "\n",
    "# 실행 예시\n",
    "fps, pca, X = mfpca_via_univariate(fd_smooth,\n",
    "                                   n_univ=8,\n",
    "                                   n_global=30)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 3-1) train/test split\n",
    "idx = np.arange(len(y_data))\n",
    "tr, te, y_tr, y_te = train_test_split(idx, y_data, test_size=0.2,\n",
    "                                      stratify=y_data, random_state=42)\n",
    "X_tr, X_te = X[tr], X[te]\n",
    "\n",
    "# 3-2) RidgeClassifierCV (또는 원하는 분류기)\n",
    "clf = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "clf.fit(X_tr, y_tr)\n",
    "\n",
    "# 3-3) 평가\n",
    "y_pred = clf.predict(X_te)\n",
    "print(\"Accuracy:\", accuracy_score(y_te, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6152e05e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FDataGrid' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m n_components = \u001b[32m15\u001b[39m\n\u001b[32m     27\u001b[39m mfpca = MFPCA(n_components=n_components)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mmfpca\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfd_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 각 sample을 주성분 점수(feature)로 변환\u001b[39;00m\n\u001b[32m     31\u001b[39m X_train_scores = mfpca.transform(fd_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv2/lib/python3.12/site-packages/FDApy/preprocessing/dim_reduction/mfpca.py:508\u001b[39m, in \u001b[36mMFPCA.fit\u001b[39m\u001b[34m(self, data, points, method_smoothing, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Estimate the eigencomponents of the data.\u001b[39;00m\n\u001b[32m    481\u001b[39m \n\u001b[32m    482\u001b[39m \u001b[33;03mBefore estimating the eigencomponents, the data is centered. Using the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    499\u001b[39m \n\u001b[32m    500\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m points \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    502\u001b[39m     points = [\n\u001b[32m    503\u001b[39m         (\n\u001b[32m    504\u001b[39m             data_univariate.argvals\n\u001b[32m    505\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_univariate, DenseFunctionalData)\n\u001b[32m    506\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m data_univariate.argvals.to_dense()\n\u001b[32m    507\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m data_univariate \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\n\u001b[32m    509\u001b[39m     ]\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    511\u001b[39m     \u001b[38;5;28mself\u001b[39m.weights = np.repeat(\u001b[32m1\u001b[39m, data.n_functional)\n",
      "\u001b[31mAttributeError\u001b[39m: 'FDataGrid' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "# 1) 세그먼트 로드: multivariate (모든 관절 xyz)\n",
    "segment_dir = \"/Users/junokim/Desktop/jupyteryong2/slr/peak_detection_results/segments2\"\n",
    "# load_segmented_time_series: 각각 (timepoints, features) 형태로 반환\n",
    "x_data, y_data = load_segmented_time_series(segment_dir, max_len=11)\n",
    "\n",
    "# x_data.shape == (n_samples, 11, n_features)\n",
    "# 2) FDataGrid 생성 (multivariate codomain)\n",
    "grid = np.linspace(0, 1, 11)              # 시간 정규화\n",
    "fd = FDataGrid(data_matrix=x_data,       # (n_samples, timepoints, features)\n",
    "               grid_points=[grid])       # 단일 도메인\n",
    "\n",
    "# 3) 스무딩 (옵션)\n",
    "basis    = BSplineBasis(n_basis=9)\n",
    "smoother = BasisSmoother(basis=basis)\n",
    "fd_smooth = smoother.fit_transform(fd)\n",
    "\n",
    "# 4) 학습/테스트 분리\n",
    "idx = np.arange(len(y_data))\n",
    "train_idx, test_idx, y_train, y_test = train_test_split(\n",
    "    idx, y_data, test_size=0.2, stratify=y_data, random_state=42\n",
    ")\n",
    "fd_train = fd_smooth[train_idx]\n",
    "fd_test  = fd_smooth[test_idx]\n",
    "\n",
    "# 5) MFPCA + RidgeClassifier 학습\n",
    "n_components = 15\n",
    "mfpca = MFPCA(n_components=n_components)\n",
    "mfpca.fit(fd_train)\n",
    "\n",
    "# 각 sample을 주성분 점수(feature)로 변환\n",
    "X_train_scores = mfpca.transform(fd_train)\n",
    "X_test_scores  = mfpca.transform(fd_test)\n",
    "\n",
    "# 분류기 학습\n",
    "clf = RidgeClassifierCV(alphas=np.logspace(-3,3,10))\n",
    "clf.fit(X_train_scores, y_train)\n",
    "\n",
    "# 6) 평가\n",
    "y_pred = clf.predict(X_test_scores)\n",
    "print(\"▶ Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"▶ Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"▶ Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# 7) 저장\n",
    "model = {\"mfpca\": mfpca, \"classifier\": clf}\n",
    "with open(\"lateralraise_mfpca.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
